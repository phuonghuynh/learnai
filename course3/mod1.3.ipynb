{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T05:36:05.793051Z",
     "start_time": "2025-06-12T05:36:05.780192Z"
    }
   },
   "source": [
    "# Install modules\n",
    "# A '!' in a Jupyter Notebook runs the line in the system's shell, and not in the Python interpreter\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "# you can download this dataset from https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/tree/main\n",
    "data = pd.read_csv('./test.csv')\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              tweet    class  \\\n",
      "0  31079  Rear garden alarm activated  at Telford house ...  sadness   \n",
      "1  11428  @bt_uk appointment booked between 1-6 today, w...     fear   \n",
      "2  21865  Hoping to hear some serious @DonnieTrumpet ton...     fear   \n",
      "3  31125  I'm always smiling so that's why I'm always ha...  sadness   \n",
      "4  30912  @Sirenja_ @JaxOfBo (like, hope i didn't offend...      joy   \n",
      "\n",
      "   sentiment_intensity class_intensity  labels  \n",
      "0                  NaN    sadness_NONE      -1  \n",
      "1                  NaN       fear_NONE      -1  \n",
      "2                  NaN       fear_NONE      -1  \n",
      "3                  NaN    sadness_NONE      -1  \n",
      "4                  NaN        joy_NONE      -1  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:36:23.959168Z",
     "start_time": "2025-06-12T05:36:23.938151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re # Import the `re` module for working with regular expressions\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # Convert all text to lowercase for uniformity\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs from the text\n",
    "    text = re.sub(r'<.*?>', '', text) # Remove any HTML tags from the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation, keep only words and spaces\n",
    "    return text # Return the cleaned text\n",
    "\n",
    "# Assume `data` is a pandas DataFrame with a column named 'text'\n",
    "# Apply the cleaning function to each row of the 'text' column\n",
    "data['cleaned_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "# Print the first 5 rows of the cleaned text to verify the cleaning process\n",
    "print(data['cleaned_text'].head())"
   ],
   "id": "657db6644318818e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    rear garden alarm activated  at telford house ...\n",
      "1    bt_uk appointment booked between 16 today wait...\n",
      "2    hoping to hear some serious donnietrumpet tonight\n",
      "3      im always smiling so thats why im always happy \n",
      "4    sirenja_ jaxofbo like hope i didnt offend with...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:36:36.997703Z",
     "start_time": "2025-06-12T05:36:36.990757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values in the dataset\n",
    "print(data.isnull().sum()) # Print the count of missing values for each column\n",
    "\n",
    "# Option 1: Remove rows with missing data in the 'cleaned_text' column\n",
    "data = data.dropna(subset=['cleaned_text']) # Drop rows where 'cleaned_text' is NaN (missing)\n",
    "\n",
    "# Option 2: Fill missing values in 'cleaned_text' with a placeholder\n",
    "data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'"
   ],
   "id": "27abcca23166fc78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "tweet                     0\n",
      "class                     0\n",
      "sentiment_intensity    3142\n",
      "class_intensity           0\n",
      "labels                    0\n",
      "cleaned_text              0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/7p1ldf8d4zj16svxk9h0h34c0000gn/T/ipykernel_5308/3840859666.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['cleaned_text'].fillna('unknown', inplace=True) # Replace NaN values in 'cleaned_text' with 'unknown'\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:38:05.254502Z",
     "start_time": "2025-06-12T05:37:59.691698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(tokens['input_ids'][:5])  # Preview the first 5 tokenized examples"
   ],
   "id": "617b34a78f03a553",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phuonghqh/.pyenv/versions/learnai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  4373,  3871,  8598,  8878,  2012, 10093,  3877,  2160,  2012,\n",
      "         10114,  2549,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18411,  1035,  2866,  6098, 17414,  2090,  2385,  2651,  4741,\n",
      "          1999,  2035,  2154,  1998,  6343,  3662,  2039,  2036,  7303,  1037,\n",
      "          2655,  2067,  1998,  2196,  2288,  2028,  9643,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5327,  2000,  2963,  2070,  3809, 28486, 24456, 22327,  3892,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101, 10047,  2467,  5629,  2061,  2008,  2015,  2339, 10047,  2467,\n",
      "          3407,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101, 19558,  3900,  1035, 13118, 11253,  5092,  2066,  3246,  1045,\n",
      "          2134,  2102,  2125, 10497,  2007,  2026,  8570,  2009,  2347,  2102,\n",
      "          2054,  1045,  2001, 16533,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:39:26.595887Z",
     "start_time": "2025-06-12T05:39:20.683242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "# Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "\n",
    "# If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "# Select a random synonym and get the first lemma (word form) of that synonym\n",
    "        return random.choice(synonyms).lemmas()[0].name()\n",
    "\n",
    "# If no synonyms are found, return the original word\n",
    "    return word\n",
    "\n",
    "# Define a function to augment text by replacing words with synonyms randomly\n",
    "def augment_text(text):\n",
    "# Split the input text into individual words\n",
    "    words = text.split() # Split the input text into individual words\n",
    "\n",
    "# Replace each word with a synonym with a probability of 20% (random.random() > 0.8)\n",
    "    augmented_words = [\n",
    "    synonym_replacement(word) if random.random() > 0.8 else word\n",
    "# If random condition met, replace\n",
    "for word in words] # Iterate over each word in the original text\n",
    "\n",
    "# Join the augmented words back into a single string and return it\n",
    "    return ' '.join(augmented_words)\n",
    "\n",
    "# Apply the text augmentation function to the 'cleaned_text' column in a DataFrame\n",
    "# Create a new column 'augmented_text' containing the augmented version of 'cleaned_text'\n",
    "data['augmented_text'] = data['cleaned_text'].apply(augment_text)"
   ],
   "id": "69d93c7ecf6e412b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/phuonghqh/.pyenv/versions/3.12.10/envs/learnai/lib/python3.12/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: click in /Users/phuonghqh/.pyenv/versions/3.12.10/envs/learnai/lib/python3.12/site-packages (from nltk) (8.2.1)\r\n",
      "Requirement already satisfied: joblib in /Users/phuonghqh/.pyenv/versions/3.12.10/envs/learnai/lib/python3.12/site-packages (from nltk) (1.5.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/phuonghqh/.pyenv/versions/3.12.10/envs/learnai/lib/python3.12/site-packages (from nltk) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /Users/phuonghqh/.pyenv/versions/3.12.10/envs/learnai/lib/python3.12/site-packages (from nltk) (4.67.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/phuonghqh/nltk_data...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:41:27.186336Z",
     "start_time": "2025-06-12T05:41:27.179601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch # Import PyTorch library\n",
    "from torch.utils.data import TensorDataset, DataLoader # Import modules to create datasets and data loaders\n",
    "\n",
    "# Convert tokenized data into PyTorch tensors\n",
    "# input_ids = tokens['input_ids'] # Extract input IDs from the tokenized data\n",
    "# attention_masks = tokens['attention_mask'] # Extract attention masks from the tokenized data\n",
    "input_ids = torch.tensor(tokens['input_ids'])\n",
    "attention_masks = torch.tensor(tokens['attention_mask'])\n",
    "\n",
    "# Define a mapping function\n",
    "def map_sentiment(value):\n",
    "    if value == \"high\":\n",
    "        return 1\n",
    "    elif value == \"medium\":\n",
    "        return 0.5\n",
    "    elif value == \"low\":\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Handle unexpected values, if any\n",
    "\n",
    "# Apply the function to each item in 'sentiment_intensity'\n",
    "data['sentiment_intensity'] = data['sentiment_intensity'].apply(map_sentiment)\n",
    "\n",
    "# Drop any rows where 'sentiment_intensity' is None\n",
    "data = data.dropna(subset=['sentiment_intensity']).reset_index(drop=True)\n",
    "\n",
    "# Convert the 'sentiment_intensity' column to a tensor\n",
    "labels = torch.tensor(data['sentiment_intensity'].tolist())"
   ],
   "id": "410b9755b931315d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7t/7p1ldf8d4zj16svxk9h0h34c0000gn/T/ipykernel_5308/2264232787.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(tokens['input_ids'])\n",
      "/var/folders/7t/7p1ldf8d4zj16svxk9h0h34c0000gn/T/ipykernel_5308/2264232787.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_masks = torch.tensor(tokens['attention_mask'])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:43:48.094314Z",
     "start_time": "2025-06-12T05:43:48.030378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split # Import function to split dataset\n",
    "\n",
    "# First split: 15% for test set, the rest for training/validation\n",
    "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 20% for validation set from remaining data\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDataset objects for each set, including attention masks\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"Training, validation, and test sets are prepared with attention masks!\")"
   ],
   "id": "9d1b82742704bda3",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3142, 3142, 0]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_selection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_test_split \u001B[38;5;66;03m# Import function to split dataset\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# First split: 15% for test set, the rest for training/validation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_masks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m42\u001B[39;49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Second split: 20% for validation set from remaining data\u001B[39;00m\n\u001B[32m      9\u001B[39m train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n\u001B[32m     10\u001B[39m     train_val_inputs, train_val_masks, train_val_labels, test_size=\u001B[32m0.2\u001B[39m, random_state=\u001B[32m42\u001B[39m\n\u001B[32m     11\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    214\u001B[39m         skip_parameter_validation=(\n\u001B[32m    215\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    216\u001B[39m         )\n\u001B[32m    217\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    224\u001B[39m     msg = re.sub(\n\u001B[32m    225\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    226\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    227\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    228\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2916\u001B[39m, in \u001B[36mtrain_test_split\u001B[39m\u001B[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[39m\n\u001B[32m   2913\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_arrays == \u001B[32m0\u001B[39m:\n\u001B[32m   2914\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mAt least one array required as input\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m2916\u001B[39m arrays = \u001B[43mindexable\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2918\u001B[39m n_samples = _num_samples(arrays[\u001B[32m0\u001B[39m])\n\u001B[32m   2919\u001B[39m n_train, n_test = _validate_shuffle_split(\n\u001B[32m   2920\u001B[39m     n_samples, test_size, train_size, default_test_size=\u001B[32m0.25\u001B[39m\n\u001B[32m   2921\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/sklearn/utils/validation.py:530\u001B[39m, in \u001B[36mindexable\u001B[39m\u001B[34m(*iterables)\u001B[39m\n\u001B[32m    500\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[32m    501\u001B[39m \n\u001B[32m    502\u001B[39m \u001B[33;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    526\u001B[39m \u001B[33;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001B[39;00m\n\u001B[32m    527\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    529\u001B[39m result = [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[32m--> \u001B[39m\u001B[32m530\u001B[39m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    531\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/sklearn/utils/validation.py:473\u001B[39m, in \u001B[36mcheck_consistent_length\u001B[39m\u001B[34m(*arrays)\u001B[39m\n\u001B[32m    471\u001B[39m lengths = [_num_samples(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m arrays \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[32m    472\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(lengths)) > \u001B[32m1\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m473\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    474\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    475\u001B[39m         % [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[32m    476\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Found input variables with inconsistent numbers of samples: [3142, 3142, 0]"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

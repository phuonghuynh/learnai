{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T05:11:51.021491Z",
     "start_time": "2025-06-12T05:11:51.008495Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "# Example dataset (replace with your own if you want)\n",
    "data_dict = {\n",
    "    \"text\": [\n",
    "        \"  The staff was very kind and attentive to my needs!!!  \",\n",
    "        \"The waiting time was too long, and the staff was rude. Visit us at http://hospitalreviews.com\",\n",
    "        \"The doctor answered all my questions...but the facility was outdated.   \",\n",
    "        \"The nurse was compassionate & made me feel comfortable!! :) \",\n",
    "        \"I had to wait over an hour before being seen.  Unacceptable service! #frustrated\",\n",
    "        \"The check-in process was smooth, but the doctor seemed rushed. Visit https://feedback.com\",\n",
    "        \"Everyone I interacted with was professional and helpful.  \"\n",
    "    ],\n",
    "    \"label\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
    "}\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Clean the text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove extra spaces\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "data[\"cleaned_text\"] = data[\"text\"].apply(clean_text)\n",
    "\n",
    "# Convert labels to numerical values\n",
    "data[\"label\"] = data[\"label\"].astype(\"category\").cat.codes  # Converts [\"positive\", \"negative\", \"neutral\"] to [0, 1, 2]\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "data['tokenized'] = data['cleaned_text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# Pad or truncate to fixed length (e.g., 128 tokens)\n",
    "data['padded_tokenized'] = data['tokenized'].apply(\n",
    "    lambda x: x + [tokenizer.pad_token_id] * (128 - len(x)) if len(x) < 128 else x[:128]\n",
    ")\n",
    "\n",
    "# Preview cleaned and labeled data\n",
    "print(data[['cleaned_text', 'label', 'padded_tokenized']].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0    The staff was very kind and attentive to my ...      2   \n",
      "1  The waiting time was too long, and the staff w...      0   \n",
      "2  The doctor answered all my questions...but the...      1   \n",
      "3  The nurse was compassionate & made me feel com...      2   \n",
      "4  I had to wait over an hour before being seen. ...      0   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  the staff was very kind and attentive to my needs  \n",
      "1  the waiting time was too long and the staff wa...  \n",
      "2  the doctor answered all my questionsbut the fa...  \n",
      "3  the nurse was compassionate  made me feel comf...  \n",
      "4  i had to wait over an hour before being seen  ...  \n",
      "                                        cleaned_text  label  \\\n",
      "0  the staff was very kind and attentive to my needs      2   \n",
      "1  the waiting time was too long and the staff wa...      0   \n",
      "2  the doctor answered all my questionsbut the fa...      1   \n",
      "3  the nurse was compassionate  made me feel comf...      2   \n",
      "4  i had to wait over an hour before being seen  ...      0   \n",
      "\n",
      "                                    padded_tokenized  \n",
      "0  [101, 1996, 3095, 2001, 2200, 2785, 1998, 2012...  \n",
      "1  [101, 1996, 3403, 2051, 2001, 2205, 2146, 1998...  \n",
      "2  [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569...  \n",
      "3  [101, 1996, 6821, 2001, 29353, 2081, 2033, 251...  \n",
      "4  [101, 1045, 2018, 2000, 3524, 2058, 2019, 3178...  \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:10:02.830861Z",
     "start_time": "2025-06-12T05:10:00.709438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Apply tokenization with padding\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Apply tokenization\n",
    "data[\"tokenized\"] = data[\"cleaned_text\"].apply(tokenize_function)\n",
    "\n",
    "# Extract tokenized features\n",
    "data[\"input_ids\"] = data[\"tokenized\"].apply(lambda x: x[\"input_ids\"])\n",
    "data[\"attention_mask\"] = data[\"tokenized\"].apply(lambda x: x[\"attention_mask\"])\n",
    "\n",
    "# Drop old tokenized column\n",
    "data = data.drop(columns=[\"tokenized\"])\n",
    "\n",
    "print(data.head())"
   ],
   "id": "f7d591884e8b9003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0    The staff was very kind and attentive to my ...      2   \n",
      "1  The waiting time was too long, and the staff w...      0   \n",
      "2  The doctor answered all my questions...but the...      1   \n",
      "3  The nurse was compassionate & made me feel com...      2   \n",
      "4  I had to wait over an hour before being seen. ...      0   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  the staff was very kind and attentive to my needs   \n",
      "1  the waiting time was too long and the staff wa...   \n",
      "2  the doctor answered all my questionsbut the fa...   \n",
      "3  the nurse was compassionate  made me feel comf...   \n",
      "4  i had to wait over an hour before being seen  ...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 1996, 3095, 2001, 2200, 2785, 1998, 2012...   \n",
      "1  [101, 1996, 3403, 2051, 2001, 2205, 2146, 1998...   \n",
      "2  [101, 1996, 3460, 4660, 2035, 2026, 3980, 8569...   \n",
      "3  [101, 1996, 6821, 2001, 29353, 2081, 2033, 251...   \n",
      "4  [101, 1045, 2018, 2000, 3524, 2058, 2019, 3178...   \n",
      "\n",
      "                                      attention_mask  \n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...  \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T05:10:03.067990Z",
     "start_time": "2025-06-12T05:10:02.846890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fine tune model\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"text\", \"cleaned_text\"])\n",
    "test_dataset = test_dataset.remove_columns([\"text\", \"cleaned_text\"])\n",
    "\n",
    "#print(train_dataset)\n",
    "\n",
    "# Enable dynamic padding for batches\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "# Load pre-trained BERT model (3-class classification)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "#correctly\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "70eff4231b52c78e",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m#print(train_dataset)\u001B[39;00m\n\u001B[32m     17\u001B[39m \n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Enable dynamic padding for batches\u001B[39;00m\n\u001B[32m     19\u001B[39m data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m training_args = \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./results\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlogging_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./logs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreport_to\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mnone\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mepoch\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#evaluation_strategy=\"epoch\",\u001B[39;49;00m\n\u001B[32m     30\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# Load pre-trained BERT model (3-class classification)\u001B[39;00m\n\u001B[32m     32\u001B[39m model = AutoModelForSequenceClassification.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mbert-base-uncased\u001B[39m\u001B[33m\"\u001B[39m, num_labels=\u001B[32m3\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:131\u001B[39m, in \u001B[36m__init__\u001B[39m\u001B[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/transformers/training_args.py:1738\u001B[39m, in \u001B[36mTrainingArguments.__post_init__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1736\u001B[39m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[32m   1737\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_available():\n\u001B[32m-> \u001B[39m\u001B[32m1738\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m   1740\u001B[39m \u001B[38;5;66;03m# Disable average tokens when using single device\u001B[39;00m\n\u001B[32m   1741\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.average_tokens_across_devices:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/transformers/training_args.py:2268\u001B[39m, in \u001B[36mTrainingArguments.device\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2264\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2265\u001B[39m \u001B[33;03mThe device used by this process.\u001B[39;00m\n\u001B[32m   2266\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2267\u001B[39m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[33m\"\u001B[39m\u001B[33mtorch\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m-> \u001B[39m\u001B[32m2268\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_setup_devices\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/transformers/utils/generic.py:67\u001B[39m, in \u001B[36mcached_property.__get__\u001B[39m\u001B[34m(self, obj, objtype)\u001B[39m\n\u001B[32m     65\u001B[39m cached = \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     cached = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/learnai/lib/python3.12/site-packages/transformers/training_args.py:2138\u001B[39m, in \u001B[36mTrainingArguments._setup_devices\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[32m   2137\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[32m-> \u001B[39m\u001B[32m2138\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m   2139\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`: \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2140\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPlease run `pip install transformers[torch]` or `pip install \u001B[39m\u001B[33m'\u001B[39m\u001B[33maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2141\u001B[39m         )\n\u001B[32m   2142\u001B[39m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[32m   2143\u001B[39m accelerator_state_kwargs = {\u001B[33m\"\u001B[39m\u001B[33menabled\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33muse_configured_state\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[31mImportError\u001B[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Generate predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = test_dataset['label']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")"
   ],
   "id": "dde0195336169216"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
